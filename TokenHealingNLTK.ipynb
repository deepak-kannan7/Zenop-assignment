{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deepak-kannan7/Token-Healing-in-Python/blob/main/TokenHealingNLTK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLcXil5-OKtB",
        "outputId": "3135b79d-45e1-4dd3-ffda-1645f5d2874a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('words')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Q5LnvgHVzgS",
        "outputId": "5cffd58b-f3bf-41a9-e098-8c5959ee42f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I aam a artificial intelligence engineer\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import words\n",
        "from nltk.tokenize import word_tokenize\n",
        "def is_word(token):\n",
        "    return token.lower() in words.words()\n",
        "def token_healing(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    healed_tokens = []\n",
        "    for token in tokens:\n",
        "        if is_word(token):\n",
        "            healed_tokens.append(token)\n",
        "        else:\n",
        "            sub_tokens = {word: nltk.edit_distance(token, word) for word in words.words()}\n",
        "            best_match = min(sub_tokens, key=sub_tokens.get)\n",
        "            healed_tokens.append(best_match)\n",
        "    healed_text = ' '.join(healed_tokens)\n",
        "    return healed_text\n",
        "input_text = \"I amm a articifial intellicegnce egnineer\"\n",
        "healed_text = token_healing(input_text)\n",
        "print(healed_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install fuzzywuzzy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NR7BjILptKtA",
        "outputId": "775bda9a-6b33-4bdb-eed3-6ee00d3eb58e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fuzzywuzzy\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Installing collected packages: fuzzywuzzy\n",
            "Successfully installed fuzzywuzzy-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import words\n",
        "from nltk.tokenize import word_tokenize\n",
        "from fuzzywuzzy import fuzz\n",
        "from fuzzywuzzy import process\n",
        "\n",
        "def is_word(token):\n",
        "    return token.lower() in words.words()\n",
        "\n",
        "def get_best_match(token):\n",
        "    best_match, _ = process.extractOne(token, words.words(), scorer=fuzz.ratio)\n",
        "    return best_match\n",
        "\n",
        "def token_healing(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    healed_tokens = []\n",
        "\n",
        "    for token in tokens:\n",
        "        if is_word(token):\n",
        "            healed_tokens.append(token)\n",
        "        else:\n",
        "            best_match = get_best_match(token)\n",
        "            healed_tokens.append(best_match)\n",
        "\n",
        "    healed_text = ' '.join(healed_tokens)\n",
        "    return healed_text\n",
        "\n",
        "input_text = \"I is a articifal ingelltence enthutisat\"\n",
        "healed_text = token_healing(input_text)\n",
        "print(healed_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMJ2Jig7tOtY",
        "outputId": "a55f7a20-073f-427c-d6e6-78f0a184bdda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I is a artificial inexcellence Ehatisaht\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install language-tool-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUtGQyxjvDkj",
        "outputId": "db3eeebf-4176-4467-b796-d440a47ed888"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting language-tool-python\n",
            "  Downloading language_tool_python-2.7.1-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from language-tool-python) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from language-tool-python) (4.65.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->language-tool-python) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->language-tool-python) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->language-tool-python) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->language-tool-python) (3.4)\n",
            "Installing collected packages: language-tool-python\n",
            "Successfully installed language-tool-python-2.7.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import language_tool_python\n",
        "\n",
        "def token_healing(text):\n",
        "    tool = language_tool_python.LanguageTool('en-US')\n",
        "\n",
        "    tokens = text.split()\n",
        "    healed_tokens = []\n",
        "\n",
        "    for token in tokens:\n",
        "        if tool.check(token):\n",
        "            corrected_tokens = tool.correct(token)\n",
        "            if len(corrected_tokens) > 0:\n",
        "                best_match = corrected_tokens[0]\n",
        "                healed_tokens.append(best_match)\n",
        "            else:\n",
        "                healed_tokens.append(token)\n",
        "        else:\n",
        "            healed_tokens.append(token)\n",
        "\n",
        "    healed_text = ' '.join(healed_tokens)\n",
        "    return healed_text\n",
        "\n",
        "input_text = \"I is a human.\"\n",
        "healed_text = token_healing(input_text)\n",
        "print(healed_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BswwFvs8vKqd",
        "outputId": "402b1207-7ba7-433f-cdfa-4bfeea789f25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I is a H\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOF62qgrmTlMt6eojjArMGN",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}